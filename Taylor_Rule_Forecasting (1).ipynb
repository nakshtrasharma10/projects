{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b61b6df-c842-419c-984d-4f0b2acacdd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msms\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformula\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmf\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fitter \u001b[38;5;66;03m# might require install, numpy version matters\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_val_score, KFold\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fitter'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import random; random.seed(10) # pre-setting seed\n",
    "from scipy import stats\n",
    "from scipy import optimize  # for Box-Cox calculations\n",
    "from scipy.stats import norm\n",
    "from matplotlib import rcParams # for ease of resizing plots\n",
    "# Numpy version matters for scipy\n",
    "\n",
    "# For model fitting\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "from fitter import Fitter # might require install, numpy version matters\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e0600-0c67-4edd-b1cf-9dc68bb95805",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xlrd==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584e9f5-6a3b-4e3f-a40e-29a09880cc3b",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb469d-aeb1-4d92-9eb6-58fb7d875558",
   "metadata": {},
   "outputs": [],
   "source": [
    "Core_PCE = pd.read_csv(\"C:/Users/naksh/OneDrive/Documents/Quarterly Core PCE Data.csv\", parse_dates = True, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550221b-f6fc-4dd1-bff3-128cb4d8f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "Core_PCE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345dc53-0530-4bfb-a4a5-32395f76122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Core_PCE.rename(columns={'Core PCE': 'Core_PCE'}, inplace=True)\n",
    "Core_PCE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6026202-73e9-40ed-a23b-b024c3ed3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual_GDP = pd.read_csv(\"C:/Users/naksh/Downloads/GDPC1 (1).csv\", parse_dates = True, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d832d2-0659-4534-8f7a-cd69d39e4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual_GDP.rename(columns={'GDPC1': 'Actual_Real_GDP'}, inplace=True)\n",
    "Actual_GDP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066526f-761a-4b34-87e3-60f76afc26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Potential_GDP = pd.read_excel(\"C:/Users/naksh/Downloads/GDPPOT.xls\", engine = 'xlrd',parse_dates = True, index_col = 0)\n",
    "Potential_GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86a159-f2f7-4345-bc83-0ba45d59288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Potential_GDP.rename(columns={'GDPPOT': 'Potential_Real_GDP'}, inplace=True)\n",
    "Potential_GDP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f86ce0-2135-492a-9b88-42e5745022f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fed_Funds = pd.read_csv(\"C:/Users/naksh/OneDrive/Documents/Quarterly.csv\", parse_dates = True, index_col = 0)\n",
    "Fed_Funds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0d15d-20d8-4d6c-81d1-b65eb1d1d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining all the importing datasets into a single dataframe\n",
    "Taylor_Rule = pd.concat([Core_PCE, Actual_GDP, Potential_GDP, Fed_Funds], axis=1)\n",
    "Taylor_Rule.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8d439-26bd-4e83-9088-81df1ade399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Taylor_Rule.dropna(inplace = True)\n",
    "Taylor_Rule.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db278cdc-e0ee-4767-94be-5eba58aa5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Taylor_Rule[['Actual_Real_GDP', 'Potential_Real_GDP']] = Taylor_Rule[['Actual_Real_GDP', 'Potential_Real_GDP']].round(2)\n",
    "Taylor_Rule.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60edfbdc-5fc5-4598-9512-e787089fcccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding a new variable called Output Gap:\n",
    "Taylor_Rule['Output_Gap'] = ((Taylor_Rule['Actual_Real_GDP'] - Taylor_Rule['Potential_Real_GDP']) / Taylor_Rule['Potential_Real_GDP']) * 100\n",
    "Taylor_Rule.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66283817-8a50-4430-a5c8-1f70e065e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a backup of the Core PCE series\n",
    "original_core_pce = Taylor_Rule['Core_PCE'].copy()\n",
    "\n",
    "# Run your structural break tests here (ruptures, Zivot-Andrews, etc.)\n",
    "\n",
    "# If needed, restore the original Core PCE values after the tests\n",
    "Taylor_Rule['Original_Core_PCE'] = original_core_pce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839a90a-961d-4797-9f7c-901da83a4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Taylor_Rule.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2cc29-a17c-435a-b62e-0cf0576f5838",
   "metadata": {},
   "source": [
    "## Stationarity and Structural Breaks Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4cf7ef-b3e1-4f75-b643-a36fe26d1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADF test for Stationarity:\n",
    "from statsmodels.tsa.stattools import adfuller, kpss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ffea7-30cf-4107-8dc3-031383d7c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = adfuller(Taylor_Rule['Core_PCE'])\n",
    "\n",
    "# Extract and display the results\n",
    "print('ADF Statistic:', result_1[0])\n",
    "print('p-value:', result_1[1])\n",
    "print('Critical Values:', result_1[4])\n",
    "\n",
    "# Interpret the result\n",
    "if result_1[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary\")\n",
    "\n",
    "## Null hypothesis here is that the series is stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae373d-25a9-4e2f-a691-fa0798c10662",
   "metadata": {},
   "source": [
    "## Bai Perron Test\n",
    "Financial time series generally tend to have structural breaks in whose presence ADF fails. Hence first we run a Bai-Perron test to check presence of multiple structural breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3623e-7c34-4daa-98f5-878649414076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruptures as rpt\n",
    "model = \"l2\"\n",
    "y = Taylor_Rule['Core_PCE'].values\n",
    "algo = rpt.Binseg(model=model).fit(y)\n",
    "\n",
    "# Testing for Structural Breaks\n",
    "result = algo.predict(n_bkps=3)  # n_bkps represents the number of breakpoints to detect\n",
    "\n",
    "# Displaying the breaks\n",
    "print(f'Detected breakpoints: {result}')\n",
    "\n",
    "# Visualizing the breaks\n",
    "rpt.display(y, result)\n",
    "plt.title('Detected Structural Breaks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126e4c3-96be-42e2-a5b6-772ae36cdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import zivot_andrews\n",
    "\n",
    "y = Taylor_Rule['Core_PCE']\n",
    "\n",
    "\n",
    "result = zivot_andrews(y, maxlag=12, regression='c')\n",
    "\n",
    "# Display the results\n",
    "print(f'Zivot-Andrews test statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'Critical values: {result[2]}')\n",
    "print(f'Break location (index): {result[3]}')\n",
    "\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary with a structural break\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary even with a structural break\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222b0fd-4059-44d9-bb92-9a7b37858711",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = adfuller(Taylor_Rule['Actual_Real_GDP'])\n",
    "\n",
    "#displaying the results\n",
    "print('ADF Statistic:', result_2[0])\n",
    "print('p-value:', result_2[1])\n",
    "print('Critical Values:', result_1[4])\n",
    "\n",
    "\n",
    "if result_2[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f62d1-89c6-4f00-a2fd-ee9b1c84e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"l2\"\n",
    "y_1 = Taylor_Rule['Actual_Real_GDP'].values\n",
    "algo = rpt.Binseg(model=model).fit(y_1)\n",
    "\n",
    "# Testing for Structural Breaks\n",
    "result = algo.predict(n_bkps=3)  # 'n_bkps' is the number of breakpoints to detect\n",
    "\n",
    "# Displaying the breaks\n",
    "print(f'Detected breakpoints: {result}')\n",
    "\n",
    "# Visualizing the breakpoints\n",
    "rpt.display(y_1, result)\n",
    "plt.title('Detected Structural Breaks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f330484-7b07-4d3c-9da9-5084c46a95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = Taylor_Rule['Actual_Real_GDP']\n",
    "\n",
    "result = zivot_andrews(y, maxlag=12, regression='c')\n",
    "\n",
    "\n",
    "print(f'Zivot-Andrews test statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'Critical values: {result[2]}')\n",
    "print(f'Break location (index): {result[3]}')\n",
    "\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary with a structural break\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary even with a structural break\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6e1b3-72e6-417e-b091-311963d33ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = adfuller(Taylor_Rule['Potential_Real_GDP'])\n",
    "\n",
    "\n",
    "print('ADF Statistic:', result_3[0])\n",
    "print('p-value:', result_3[1])\n",
    "print('Critical Values:', result_3[4])\n",
    "\n",
    "\n",
    "if result_3[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fbec0a-f239-4fc6-ad5d-45405570937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"l2\"\n",
    "y_2 = Taylor_Rule['Potential_Real_GDP'].values\n",
    "algo = rpt.Binseg(model=model).fit(y_2)\n",
    "\n",
    "# Testing for Structural Breaks\n",
    "result = algo.predict(n_bkps=3)  # 'n_bkps' is the number of breakpoints to detect\n",
    "\n",
    "# Displaying the Structural Breaks\n",
    "print(f'Detected breakpoints: {result}')\n",
    "\n",
    "# Visualizing the Structural Breaks\n",
    "rpt.display(y_2, result)\n",
    "plt.title('Detected Structural Breaks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418dcbf9-8c0d-4e9c-9108-d3d9be94a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2 = Taylor_Rule['Potential_Real_GDP']\n",
    "\n",
    "result = zivot_andrews(y, maxlag=12, regression='c')\n",
    "\n",
    "print(f'Zivot-Andrews test statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'Critical values: {result[2]}')\n",
    "print(f'Break location (index): {result[3]}')\n",
    "\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary with a structural break\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary even with a structural break\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0a122-7a81-4ea2-8d31-dd6b771e3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_4 = adfuller(Taylor_Rule['EFFR'])\n",
    "\n",
    "print('ADF Statistic:', result_4[0])\n",
    "print('p-value:', result_4[1])\n",
    "print('Critical Values:', result_4[4])\n",
    "\n",
    "if result_4[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a6542-9028-46cf-b48a-98c828272afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"l2\"\n",
    "Taylor_Rule['EFFR'] = pd.to_numeric(Taylor_Rule['EFFR'], errors='coerce')\n",
    "y_3 = Taylor_Rule['EFFR'].values\n",
    "algo = rpt.Binseg(model=model).fit(y_3)\n",
    "\n",
    "# Testing for Structural Breaks\n",
    "result = algo.predict(n_bkps=3)  # 'n_bkps' is the number of breakpoints to detect\n",
    "\n",
    "# Displaying the Structural Breaks\n",
    "print(f'Detected breakpoints: {result}')\n",
    "\n",
    "# Visualizing the Structural Breaks\n",
    "rpt.display(y_3, result)\n",
    "plt.title('Detected Structural Breaks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c67d07-ab94-4c00-b3c0-f446a99d368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_3 = Taylor_Rule['EFFR']\n",
    "\n",
    "result = zivot_andrews(y, maxlag=12, regression='c')\n",
    "\n",
    "print(f'Zivot-Andrews test statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'Critical values: {result[2]}')\n",
    "print(f'Break location (index): {result[3]}')\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary with a structural break\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary even with a structural break\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25487ee9-aec9-45e2-8617-01598bccc71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_5 = adfuller(Taylor_Rule['Output_Gap'])\n",
    "\n",
    "print('ADF Statistic:', result_5[0])\n",
    "print('p-value:', result_5[1])\n",
    "print('Critical Values:', result_5[4])\n",
    "\n",
    "if result_5[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4590f-384e-400c-867e-3645924c1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"l2\"\n",
    "y_4 = Taylor_Rule['Output_Gap'].values\n",
    "algo = rpt.Binseg(model=model).fit(y_4)\n",
    "\n",
    "## Testing for Structural Breaks\n",
    "result = algo.predict(n_bkps=3)  # 'n_bkps' is the number of breakpoints to detect\n",
    "\n",
    "## Displaying the Structural breaks\n",
    "print(f'Detected breakpoints: {result}')\n",
    "\n",
    "## Visualizing the results\n",
    "rpt.display(y_4, result)\n",
    "plt.title('Detected Structural Breaks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca6beb-cbd2-4dfa-9b84-da31f9912914",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_4 = Taylor_Rule['Output_Gap'].dropna()  # Ensure there are no missing values\n",
    "\n",
    "result = zivot_andrews(y, maxlag=12, regression='c')\n",
    "\n",
    "print(f'Zivot-Andrews test statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'Critical values: {result[2]}')\n",
    "print(f'Break location (index): {result[3]}')\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"Reject the null hypothesis - The series is stationary with a structural break\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - The series is non-stationary even with a structural break\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa72a5-3802-4061-b02e-4823ee34503c",
   "metadata": {},
   "source": [
    "## AR, MA and ARMA process checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9bbc4c-0974-4a79-9016-d695bca17f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "## ACF and PACF for sour Effective Federal Funds rate variables\n",
    "series = Taylor_Rule['EFFR']\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "plot_acf(series, lags=40, ax=ax[0])\n",
    "plot_pacf(series, lags=40, ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Autocorrelation Function (ACF) for EFFR')\n",
    "ax[1].set_title('Partial Autocorrelation Function (PACF) for EFFR')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa7825-d02f-4919-a78f-211d5142f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF and PACF for Core_PCE\n",
    "series = Taylor_Rule['Original_Core_PCE']\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "plot_acf(series, lags=40, ax=ax[0])\n",
    "plot_pacf(series, lags=40, ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Autocorrelation Function (ACF) for Core_PCE')\n",
    "ax[1].set_title('Partial Autocorrelation Function (PACF) for Core_PCE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ec67d-7f05-46e9-ab22-c5f8cad03cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF and PACF for Output Gap\n",
    "series = Taylor_Rule['Output_Gap']\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "plot_acf(series, lags=40, ax=ax[0])\n",
    "plot_pacf(series, lags=40, ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Autocorrelation Function (ACF) for Output_Gap')\n",
    "ax[1].set_title('Partial Autocorrelation Function (PACF) for Output_Gap')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378caeb-cd14-47cf-b597-e592da567e6e",
   "metadata": {},
   "source": [
    "Visually we can see the presence of AR and MA characterstics. However to be statistically sure we will now run different model specifications and compare their AIC and BIC measures of good fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586ab69-2583-4ca8-b7e2-3d672ef47888",
   "metadata": {},
   "source": [
    "## Regression Building:\n",
    "\n",
    "We now code the Multiple Regression model, once without considering the AR, MA, and ARMA processes and then accounting for these characteristics in our regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5042f86-1887-43eb-8474-ce1dbc078a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Taylor_Rule[['EFFR', 'Original_Core_PCE', 'Output_Gap']]\n",
    "\n",
    "# Calculating the Inflation Deviation (Core PCE - 2)\n",
    "df['Inflation_Deviation'] = df['Original_Core_PCE'] - 2\n",
    "\n",
    "# Defining predictors (X) and target (y) using the provided formula\n",
    "X = df[['Inflation_Deviation', 'Output_Gap']]\n",
    "y = df['EFFR']\n",
    "\n",
    "# Splitting data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Adding a constant to predictors for the intercept\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "# Fitting the MLR model using the adjusted formula\n",
    "mlr_model_taylor = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "# Printing the model summary\n",
    "print(mlr_model_taylor.summary())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_mlr_taylor = mlr_model_taylor.predict(X_test_const)\n",
    "\n",
    "# Calculating RMSE for the adjusted MLR model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_mlr_taylor = np.sqrt(mean_squared_error(y_test, y_pred_mlr_taylor))\n",
    "print(f'RMSE for Taylor Rule-Based Multiple Linear Regression: {rmse_mlr_taylor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1623e-bebb-4bb4-a151-58bc93693a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "df = Taylor_Rule[['EFFR', 'Original_Core_PCE', 'Output_Gap']]\n",
    "\n",
    "# Calculate the Inflation Deviation (Core_PCE - 2)\n",
    "df['Inflation_Deviation'] = df['Original_Core_PCE'] - 2\n",
    "\n",
    "# Ftting AR(1) model to Inflation Deviation to get residuals\n",
    "# Use ARIMA with d=0 to represent ARMA(1,0)\n",
    "arma_model = sm.tsa.ARIMA(df['Inflation_Deviation'], order=(1, 0, 0)).fit()\n",
    "\n",
    "# Creating lagged residuals from the AR(1) model for the MA(1) part\n",
    "df['Inflation_Deviation_residuals'] = arma_model.resid\n",
    "df['Inflation_Deviation_residuals_lag1'] = df['Inflation_Deviation_residuals'].shift(1)\n",
    "\n",
    "# Creatinf lagged terms for EFFR (AR(2))\n",
    "df['EFFR_lag1'] = df['EFFR'].shift(1)\n",
    "df['EFFR_lag2'] = df['EFFR'].shift(2)\n",
    "\n",
    "# Creating lagged terms for Inflation Deviation (AR(1))\n",
    "df['Inflation_Deviation_lag1'] = df['Inflation_Deviation'].shift(1)\n",
    "\n",
    "# Creating lagged terms for Output Gap (AR(1))\n",
    "df['Output_Gap_lag1'] = df['Output_Gap'].shift(1)\n",
    "\n",
    "# Dropping rows with NaN values resulting from shifts\n",
    "df = df.dropna()\n",
    "\n",
    "# De0fining predictors (X) and target (y), including ARMA(1,1) terms for Inflation Deviation\n",
    "X = df[['Inflation_Deviation', 'Inflation_Deviation_lag1', 'Inflation_Deviation_residuals_lag1', \n",
    "        'Output_Gap', 'Output_Gap_lag1', 'EFFR_lag1', 'EFFR_lag2']]\n",
    "y = df['EFFR']\n",
    "\n",
    "# Splitting data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Adding a constant to predictors for intercept\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "# Fitting the MLR model with ARMA(1,1) terms for Inflation Deviation\n",
    "mlr_model_taylor_lagged_arma = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "# Printing the model summary\n",
    "print(mlr_model_taylor_lagged_arma.summary())\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred_mlr_taylor_lagged_arma = mlr_model_taylor_lagged_arma.predict(X_test_const)\n",
    "\n",
    "# Calculating RMSE for the adjusted MLR model with ARMA(1,1) terms\n",
    "rmse_mlr_taylor_lagged_arma = np.sqrt(mean_squared_error(y_test, y_pred_mlr_taylor_lagged_arma))\n",
    "print(f'RMSE for Taylor Rule-Based Multiple Linear Regression (with ARMA(1,1) Inflation Deviation): {rmse_mlr_taylor_lagged_arma}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655a9a2-b16b-49e6-be7e-4e891e215b52",
   "metadata": {},
   "source": [
    "## Serial Correlation and Heteroskedasticity test for our MLR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f61b2-b5cb-4082-b75e-3dae50903519",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lagged MLS\n",
    "\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white, acorr_breusch_godfrey\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Extract residuals from the model\n",
    "residuals = mlr_model_taylor_lagged_arma.resid\n",
    "\n",
    "# 1. Durbin-Watson Test for Serial Correlation\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f'Durbin-Watson statistic: {dw_stat}')\n",
    "\n",
    "# 3. Breusch-Pagan Test for Heteroskedasticity\n",
    "bp_test = het_breuschpagan(residuals, mlr_model_taylor_lagged_arma.model.exog)\n",
    "print(f'Breusch-Pagan test statistic: {bp_test[0]}, p-value: {bp_test[1]}')\n",
    "\n",
    "## A value of Durbin_Watson test close to 2 tells us that there is no serial correlation \n",
    "## However, based on the low p value opf Breusch_Pagan test we reject the null of homoskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdcecf-de1d-43cd-8b52-8864c5d78d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## non Lagged MLR\n",
    "\n",
    "# Extract residuals from the model\n",
    "residuals_1 = mlr_model_taylor.resid\n",
    "\n",
    "# 1. Durbin-Watson Test for Serial Correlation\n",
    "dw_stat_1 = durbin_watson(residuals_1)\n",
    "print(f'Durbin-Watson statistic: {dw_stat_1}')\n",
    "\n",
    "# 3. Breusch-Pagan Test for Heteroskedasticity\n",
    "bp_test_1 = het_breuschpagan(residuals_1, mlr_model_taylor.model.exog)\n",
    "print(f'Breusch-Pagan test statistic: {bp_test_1[0]}, p-value: {bp_test_1[1]}')\n",
    "\n",
    "## As expected the non lagged MLR has a serial correlation problem as we have not accounted for the lagged variables\n",
    "## It also has heteroksedastic errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0c093-79c0-4221-9c26-4e3610501e62",
   "metadata": {},
   "source": [
    "## Newey West Robust Error Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbacf0f-8df6-4536-a2eb-c54704b41a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "df = Taylor_Rule[['EFFR', 'Original_Core_PCE', 'Output_Gap']]\n",
    "\n",
    "df['Inflation_Deviation'] = df['Original_Core_PCE'] - 2\n",
    "\n",
    "\n",
    "arma_model = sm.tsa.ARIMA(df['Inflation_Deviation'], order=(1, 0, 0)).fit()\n",
    "\n",
    "df['Inflation_Deviation_residuals'] = arma_model.resid\n",
    "df['Inflation_Deviation_residuals_lag1'] = df['Inflation_Deviation_residuals'].shift(1)\n",
    "\n",
    "df['EFFR_lag1'] = df['EFFR'].shift(1)\n",
    "df['EFFR_lag2'] = df['EFFR'].shift(2)\n",
    "\n",
    "df['Inflation_Deviation_lag1'] = df['Inflation_Deviation'].shift(1)\n",
    "\n",
    "df['Output_Gap_lag1'] = df['Output_Gap'].shift(1)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "X = df[['Inflation_Deviation', 'Inflation_Deviation_lag1', 'Inflation_Deviation_residuals_lag1', \n",
    "        'Output_Gap', 'Output_Gap_lag1', 'EFFR_lag1', 'EFFR_lag2']]\n",
    "y = df['EFFR']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "mlr_model_taylor_lagged_arma = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "# Apply Newey-West robust standard errors (HAC)\n",
    "# 'maxlags' defines the number of lags to consider\n",
    "nw_robust_model = mlr_model_taylor_lagged_arma.get_robustcov_results(cov_type='HAC', maxlags=4)\n",
    "\n",
    "# Print the summary of the model with Newey-West standard errors\n",
    "print(nw_robust_model.summary())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nw = nw_robust_model.predict(X_test_const)\n",
    "\n",
    "# Calculate RMSE for the adjusted MLR model with Newey-West standard errors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_nw = np.sqrt(mean_squared_error(y_test, y_pred_nw))\n",
    "print(f'RMSE for MLR with Newey-West robust standard errors: {rmse_nw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da3952b-7d4a-4042-9946-6519434b6bb0",
   "metadata": {},
   "source": [
    "Creating a VAR model in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e5149-d70d-452c-b266-6889999345fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure only the necessary variables are used for VAR\n",
    "df = df[['EFFR', 'Original_Core_PCE', 'Output_Gap']]\n",
    "\n",
    "# Splitting the data into training (80%) and testing (20%) sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]\n",
    "\n",
    "# Standardizing the training data to avoid scaling issues\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data), index=train_data.index, columns=train_data.columns)\n",
    "test_data_scaled = pd.DataFrame(scaler.transform(test_data), index=test_data.index, columns=test_data.columns)\n",
    "\n",
    "# Initialize the VAR model on the scaled training data\n",
    "model = VAR(train_data_scaled)\n",
    "\n",
    "# Set a very small number of lags (e.g., 1 or 2) to avoid instability\n",
    "optimal_lags = 1  # Start with 1 lag\n",
    "\n",
    "try:\n",
    "    var_model_fit = model.fit(optimal_lags)\n",
    "    # Print the summary of the fitted VAR model\n",
    "    print(var_model_fit.summary())\n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(f\"Error during VAR model fitting: {e}\")\n",
    "\n",
    "# Forecasting for the same number of steps as the test set length\n",
    "forecast_values = var_model_fit.forecast(y=train_data_scaled.values[-var_model_fit.k_ar:], steps=len(test_data_scaled))\n",
    "\n",
    "# Inverse transform the forecasted values to the original scale\n",
    "forecast_df_scaled = pd.DataFrame(forecast_values, index=test_data_scaled.index, \n",
    "                                  columns=['EFFR_forecast', 'Core_PCE_forecast', 'Output_Gap_forecast'])\n",
    "forecast_df = pd.DataFrame(scaler.inverse_transform(forecast_df_scaled), index=forecast_df_scaled.index,\n",
    "                           columns=forecast_df_scaled.columns)\n",
    "\n",
    "# Combining forecasted and actual values for comparison\n",
    "comparison_df = pd.concat([test_data, forecast_df], axis=1)\n",
    "print(\"Forecasted vs Actual Values:\")\n",
    "print(comparison_df.head())\n",
    "\n",
    "# Calculating the RMSE for each forecasted variable\n",
    "rmse_effr = np.sqrt(mean_squared_error(test_data['EFFR'], forecast_df['EFFR_forecast']))\n",
    "rmse_core_pce = np.sqrt(mean_squared_error(test_data['Original_Core_PCE'], forecast_df['Core_PCE_forecast']))\n",
    "rmse_output_gap = np.sqrt(mean_squared_error(test_data['Output_Gap'], forecast_df['Output_Gap_forecast']))\n",
    "\n",
    "print(f'RMSE for EFFR Forecast: {rmse_effr}')\n",
    "print(f'RMSE for Core_PCE Forecast: {rmse_core_pce}')\n",
    "print(f'RMSE for Output_Gap Forecast: {rmse_output_gap}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182890b0-8a4f-4a88-a1ce-7f69ef401dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs. forecasted values for EFFR\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_data['EFFR'], label='Actual EFFR', color='blue')\n",
    "plt.plot(forecast_df['EFFR_forecast'], label='Forecasted EFFR', color='red')\n",
    "plt.title('Actual vs Forecasted EFFR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23d75c-b011-4a80-b809-d27264620332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
